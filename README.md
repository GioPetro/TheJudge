# RAG Judge

## Project Overview

This project is an automated evaluator, the "RAG Judge," designed to assess the quality of answers generated by a Retrieval-Augmented Generation (RAG) assistant. It evaluates responses based on a set of predefined dimensions, providing both detailed, per-instance scores and aggregate statistics to help identify strengths, weaknesses, and areas for improvement in the RAG system.

## Features

*   **Automated Evaluation:** Scores RAG assistant answers without manual intervention.
*   **Multi-Dimensional Analysis:** Evaluates responses across six distinct quality dimensions.
*   **Weighted Scoring:** Combines dimensional scores into a single, comprehensive final score.
*   **CLI Interface:** Easy-to-use command-line interface for running evaluations.
*   **Detailed Reporting:** Generates human-readable Markdown reports and raw data in CSV and JSON formats.
*   **Deterministic Controls:** Allows for reproducible evaluations through temperature management.

## Evaluation Dimensions

The final score is a weighted average of the following six dimensions, each rated on a scale of 0 to 10:

| Dimension             | Weight | Description                                                              |
| --------------------- | ------ | ------------------------------------------------------------------------ |
| **Relevance**         | 25%    | How well the answer addresses the user's question.                       |
| **Groundedness**      | 25%    | How well the answer is supported by the retrieved context fragments.      |
| **Completeness**      | 15%    | Whether the answer provides a sufficient amount of information.          |
| **Factual Accuracy**  | 15%    | Whether the claims made in the answer are factually correct.             |
| **Coherence**         | 10%    | How well-structured, clear, and easy to understand the answer is.        |
| **Contextual Awareness**| 10%    | How well the answer incorporates and acknowledges the conversation history.|

### Final Score Calculation

The final score is calculated using the following formula:

```
Final Score = (Relevance * 0.25) + (Groundedness * 0.25) + (Completeness * 0.15) + (Factual Accuracy * 0.15) + (Coherence * 0.10) + (Contextual Awareness * 0.10)
```

## Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

2.  **Create a virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set up your environment variables:**
    Create a file named `.env` in the root of the project and add your Gemini API key:
    ```
    GEMINI_API_KEY="your_api_key_here"
    ```

## Usage

To run the evaluation, use the `main.py` script from the command line. For reproducible and deterministic results, use the recommended command format with temperature and seed parameters.

### Recommended Command (Deterministic Evaluation)

```bash
python main.py --csv rag_evaluation_07_2025.csv --temperature 0.0 --output reports/
```

**Note on Deterministic Evaluation:** Using `--temperature 0.0` provides the most reproducible results possible. The temperature setting of 0.0 reduces randomness in the LLM responses, ensuring consistent outputs across evaluation runs.

### Basic Command

```bash
python main.py --csv rag_evaluation_07_2025.csv --output reports/
```

### Command Line Arguments

*   `--csv`: Path to the CSV file containing the data to be evaluated (required)
*   `--output`: Directory where the output files will be saved (default: `reports/`)
*   `--temperature`: Controls randomness in LLM responses (0.0 = deterministic, 1.0 = maximum randomness)

## Output Files

The evaluation generates three types of output files in the specified output directory:

1.  **Markdown Report** (`evaluation_report_YYYYMMDD_HHMMSS.md`): Human-readable report with detailed analysis
2.  **Scored Dataset** (`scored_dataset_YYYYMMDD_HHMMSS.csv`): Original data with scores added
3.  **Aggregate Statistics** (`aggregate_stats_YYYYMMDD_HHMMSS.json`): Summary statistics and insights

## CSV Input Format

The input CSV file should contain the following columns:

*   `question`: The user's question
*   `answer`: The RAG assistant's response
*   `context`: The retrieved context fragments
*   `conversation_history`: Previous conversation context (optional)

## License

This project can only be used by the creator.